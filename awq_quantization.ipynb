{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9793d277",
   "metadata": {},
   "source": [
    "# Llama Instruct 4-bit AWQ with vLLM Compressor\n",
    "\n",
    "This notebook applies 4-bit Activation-Aware Quantization (AWQ) using the vLLM compressor (llm-compressor), serves the model with vLLM, makes a sample call, and benchmarks on 100 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b7fbb",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- GPU with compute capability >= 8.0 for fast W4A16 inference\n",
    "- Set `HUGGINGFACE_HUB_TOKEN` if the model is gated\n",
    "- Restart the kernel after installation if CUDA libraries change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62efe5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vLLM + llm-compressor and friends (pin llmcompressor 0.6.0.1: py3.12-friendly and numpy<2/vLLM compatible)\n",
    "!pip install -q --upgrade \\\n",
    "  'vllm>=0.5.5' \\\n",
    "  'llmcompressor==0.6.0.1' \\\n",
    "  'transformers>=4.43' \\\n",
    "  datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affcd2e3",
   "metadata": {},
   "source": [
    "## Configure model + paths\n",
    "AWQ needs small calibration data to compute per-channel scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975763d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "CALIBRATION_DATASET = \"HuggingFaceH4/ultrachat_200k\"\n",
    "CALIBRATION_SPLIT = \"train_sft\"\n",
    "NUM_CALIBRATION_SAMPLES = 256  # AWQ benefits from a bit more data\n",
    "MAX_SEQUENCE_LENGTH = 1024\n",
    "QUANTIZED_DIR = Path(\"llama-awq-w4a16\")\n",
    "\n",
    "QUANTIZED_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Saving quantized model to: {QUANTIZED_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412929cf",
   "metadata": {},
   "source": [
    "## Load model + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11533e50",
   "metadata": {},
   "source": [
    "## Build calibration set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = load_dataset(\n",
    "    CALIBRATION_DATASET,\n",
    "    split=f\"{CALIBRATION_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\",\n",
    ").shuffle(seed=42)\n",
    "\n",
    "def format_example(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"messages\"], tokenize=False\n",
    "        )\n",
    "    }\n",
    "\n",
    "formatted = raw_ds.map(format_example)\n",
    "\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "calibration_ds = formatted.map(tokenize, remove_columns=formatted.column_names)\n",
    "print(calibration_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f45ae",
   "metadata": {},
   "source": [
    "## Quantize to 4-bit (W4A16) with AWQ\n",
    "AWQ scales activations before weight quantization. The default mapping covers Llama-family layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.awq import AWQModifier\n",
    "\n",
    "recipe = [\n",
    "    AWQModifier(\n",
    "        ignore=[\"lm_head\"],\n",
    "        scheme=\"W4A16_ASYM\",\n",
    "        targets=[\"Linear\"],\n",
    "        duo_scaling=\"both\",\n",
    "    )\n",
    "]\n",
    "\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=calibration_ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "model.save_pretrained(QUANTIZED_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(QUANTIZED_DIR)\n",
    "print(f\"Quantized model saved to {QUANTIZED_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68977a",
   "metadata": {},
   "source": [
    "## Serve with vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcdf9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: launch the server\n",
    "# !CUDA_VISIBLE_DEVICES=0 vllm serve \"$QUANTIZED_DIR\" \\\n",
    "#   --max-model-len 4096 \\\n",
    "#   --tensor-parallel-size 1 \\\n",
    "#   --port 8000 \\\n",
    "#   --api-key dummy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6d690",
   "metadata": {},
   "source": [
    "## Call the served model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e51dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"dummy\")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"awq-w4a16-demo\",  # update to the id advertised by vLLM\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"List two advantages of AWQ.\"},\n",
    "    ],\n",
    "    max_tokens=64,\n",
    ")\n",
    "print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ecd4d",
   "metadata": {},
   "source": [
    "## Local inference without the server (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "sampling = SamplingParams(temperature=0.2, max_tokens=64)\n",
    "local_llm = LLM(model=str(QUANTIZED_DIR), tensor_parallel_size=1)\n",
    "outputs = local_llm.generate([\n",
    "    \"Explain activation-aware quantization in one sentence.\",\n",
    "], sampling)\n",
    "print(outputs[0].outputs[0].text.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce119dfc",
   "metadata": {},
   "source": [
    "## Benchmark on a small dataset (100 samples)\n",
    "Same `tweet_eval/sentiment` probe for quick accuracy-check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "LABELS = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "EVAL_SPLIT = \"train[:100]\"\n",
    "\n",
    "eval_ds = load_dataset(\"tweet_eval\", \"sentiment\", split=EVAL_SPLIT)\n",
    "\n",
    "prompts = [\n",
    "    f\"\"\"Classify the sentiment of the tweet as negative, neutral, or positive. Respond with a single word.\n",
    "\n",
    "Tweet: {row['text']}\n",
    "\n",
    "Label:\"\"\"\n",
    "    for row in eval_ds\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=3, stop=[\"\n",
    "\"])\n",
    "llm_for_eval = local_llm\n",
    "\n",
    "generations = llm_for_eval.generate(prompts, sampling_params)\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    t = text.strip().lower()\n",
    "    for lbl in LABELS.values():\n",
    "        if lbl in t:\n",
    "            return lbl\n",
    "    if t.startswith(\"pos\"):\n",
    "        return \"positive\"\n",
    "    if t.startswith(\"neg\"):\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "preds = [normalize(out.outputs[0].text) for out in generations]\n",
    "gold = [LABELS[int(row[\"label\"])] for row in eval_ds]\n",
    "\n",
    "acc = np.mean([p == g for p, g in zip(preds, gold)])\n",
    "counts = Counter(preds)\n",
    "print(f\"Accuracy on {len(gold)} samples: {acc:.3f}\")\n",
    "print(\"Prediction distribution:\", counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}