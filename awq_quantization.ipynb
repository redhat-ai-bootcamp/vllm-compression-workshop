{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9793d277",
   "metadata": {},
   "source": [
    "# Llama Instruct 4-bit AWQ with vLLM Compressor\n",
    "\n",
    "This notebook applies 4-bit Activation-Aware Quantization (AWQ) using the vLLM compressor (llm-compressor), serves the model with vLLM, makes a sample call, and benchmarks on 100 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b7fbb",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- GPU with compute capability >= 8.0 for fast W4A16 inference\n",
    "- Set `HUGGINGFACE_HUB_TOKEN` if the model is gated\n",
    "- Restart the kernel after installation if CUDA libraries change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62efe5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies from requirements.txt (run once per environment)\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q --upgrade -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up Hugging Face access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1633ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Optional: set HUGGINGFACE_HUB_TOKEN in your environment or paste it here.\n",
    "HUGGINGFACE_HUB_TOKEN = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "# HUGGINGFACE_HUB_TOKEN = \"hf_...\"  # Uncomment to hardcode for this notebook session\n",
    "\n",
    "if not HUGGINGFACE_HUB_TOKEN:\n",
    "    print(\"HUGGINGFACE_HUB_TOKEN not set; gated models may fail to load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affcd2e3",
   "metadata": {},
   "source": [
    "## 2. Configure model + paths\n",
    "AWQ needs small calibration data to compute per-channel scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "975763d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving quantized model to: /home/ubuntu/vllm-compression-workshop/llama-awq-w4a16\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "CALIBRATION_DATASET = \"HuggingFaceH4/ultrachat_200k\"\n",
    "CALIBRATION_SPLIT = \"train_sft\"\n",
    "NUM_CALIBRATION_SAMPLES = 256  # AWQ benefits from a bit more data\n",
    "MAX_SEQUENCE_LENGTH = 1024\n",
    "QUANTIZED_DIR = Path(\"llama-awq-w4a16\")\n",
    "BASE_MODEL_PATH = None  # populated by the download step below\n",
    "\n",
    "QUANTIZED_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Saving quantized model to: {QUANTIZED_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Download the base model (cache snapshot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3759e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_files_only = bool(os.getenv(\"HF_HUB_OFFLINE\")) or not HUGGINGFACE_HUB_TOKEN\n",
    "BASE_MODEL_PATH = snapshot_download(\n",
    "    MODEL_ID,\n",
    "    token=HUGGINGFACE_HUB_TOKEN,\n",
    "    local_files_only=local_files_only,\n",
    ")\n",
    "print(f\"Base model snapshot: {BASE_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412929cf",
   "metadata": {},
   "source": [
    "## 3. Load model + tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f777404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SOURCE = BASE_MODEL_PATH or MODEL_ID\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_SOURCE,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    token=HUGGINGFACE_HUB_TOKEN,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_SOURCE,\n",
    "    trust_remote_code=True,\n",
    "    token=HUGGINGFACE_HUB_TOKEN,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11533e50",
   "metadata": {},
   "source": [
    "## 3.1 Build calibration set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "481c2723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 256\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "raw_ds = load_dataset(\n",
    "    CALIBRATION_DATASET,\n",
    "    split=f\"{CALIBRATION_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\",\n",
    "    token=HUGGINGFACE_HUB_TOKEN,\n",
    ").shuffle(seed=42)\n",
    "\n",
    "def format_example(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"messages\"], tokenize=False\n",
    "        )\n",
    "    }\n",
    "\n",
    "formatted = raw_ds.map(format_example)\n",
    "\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "calibration_ds = formatted.map(tokenize, remove_columns=formatted.column_names)\n",
    "print(calibration_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f45ae",
   "metadata": {},
   "source": [
    "## 3.2 Quantize to 4-bit (W4A16) with AWQ\n",
    "AWQ scales activations before weight quantization. The default mapping covers Llama-family layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b47ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-14T19:38:01.976305+0000 | reset | INFO - Compression lifecycle reset\n",
      "2026-01-14T19:38:01.980036+0000 | from_modifiers | INFO - Creating recipe from modifiers\n",
      "2026-01-14T19:38:02.032368+0000 | on_initialize | INFO - No AWQModifier.mappings provided, inferring from model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving mapping 1/4 (0 skipped): 100%|██████████| 16/16 [00:00<00:00, 1144.50it/s]\n",
      "Resolving mapping 2/4 (15 skipped): 100%|██████████| 16/16 [00:00<00:00, 2249.41it/s]\n",
      "Resolving mapping 3/4 (0 skipped): 100%|██████████| 16/16 [00:00<00:00, 1426.09it/s]\n",
      "Resolving mapping 4/4 (0 skipped): 100%|██████████| 16/16 [00:00<00:00, 2396.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-14T19:38:02.092476+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
      "2026-01-14T19:38:02.093107+0000 | IndependentPipeline | INFO - Inferred `SequentialPipeline` for `AWQModifier`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing cache: 100%|██████████| 256/256 [00:00<00:00, 1047.98it/s]\n",
      "(1/17): Calibrating: 100%|██████████| 256/256 [00:02<00:00, 115.18it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:19<00:00,  6.49s/it]\n",
      "(1/17): Propagating: 100%|██████████| 256/256 [00:01<00:00, 199.79it/s]\n",
      "(2/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 143.31it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:19<00:00,  6.54s/it]\n",
      "(2/17): Propagating: 100%|██████████| 256/256 [00:01<00:00, 237.13it/s]\n",
      "(3/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 139.92it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:19<00:00,  6.58s/it]\n",
      "(3/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 287.04it/s]\n",
      "(4/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 140.06it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:19<00:00,  6.62s/it]\n",
      "(4/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 294.24it/s]\n",
      "(5/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 140.62it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:19<00:00,  6.66s/it]\n",
      "(5/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 295.23it/s]\n",
      "(6/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 138.79it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:20<00:00,  6.71s/it]\n",
      "(6/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 287.91it/s]\n",
      "(7/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 138.30it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:20<00:00,  6.76s/it]\n",
      "(7/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 290.38it/s]\n",
      "(8/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 138.53it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:20<00:00,  6.82s/it]\n",
      "(8/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 292.10it/s]\n",
      "(9/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 139.10it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:20<00:00,  6.89s/it]\n",
      "(9/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 292.49it/s]\n",
      "(10/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 137.07it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:20<00:00,  6.91s/it]\n",
      "(10/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 284.10it/s]\n",
      "(11/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 138.03it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:20<00:00,  6.96s/it]\n",
      "(11/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 291.01it/s]\n",
      "(12/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 139.32it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:21<00:00,  7.00s/it]\n",
      "(12/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 289.70it/s]\n",
      "(13/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 139.21it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:21<00:00,  7.04s/it]\n",
      "(13/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 290.08it/s]\n",
      "(14/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 139.77it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:21<00:00,  7.08s/it]\n",
      "(14/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 286.82it/s]\n",
      "(15/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 138.52it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:21<00:00,  7.09s/it]\n",
      "(15/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 287.61it/s]\n",
      "(16/17): Calibrating: 100%|██████████| 256/256 [00:01<00:00, 137.96it/s]\n",
      "Smoothing: 100%|██████████| 3/3 [00:21<00:00,  7.12s/it]\n",
      "(16/17): Propagating: 100%|██████████| 256/256 [00:00<00:00, 285.71it/s]\n",
      "(17/17): Calibrating: 100%|██████████| 256/256 [00:02<00:00, 94.79it/s] \n",
      "Smoothing: 0it [00:00, ?it/s]\n",
      "(17/17): Propagating: 100%|██████████| 256/256 [00:02<00:00, 101.11it/s]\n",
      "Smoothing: 0it [00:00, ?it/s]\n",
      "Calibrating weights: 100%|██████████| 327/327 [00:00<00:00, 348.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-14T19:44:24.528673+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-14T19:44:24.977709+0000 | post_process | WARNING - Optimized model is not saved. To save, please provide`output_dir` as input arg.Ex. `oneshot(..., output_dir=...)`\n",
      "2026-01-14T19:44:24.996605+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing model: 215it [00:03, 56.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to llama-awq-w4a16\n"
     ]
    }
   ],
   "source": [
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.awq import AWQModifier\n",
    "\n",
    "recipe = [\n",
    "    AWQModifier(\n",
    "        ignore=[\"lm_head\"],\n",
    "        scheme=\"W4A16_ASYM\",\n",
    "        targets=[\"Linear\"],\n",
    "        duo_scaling=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=calibration_ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "model.save_pretrained(QUANTIZED_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(QUANTIZED_DIR)\n",
    "print(f\"Quantized model saved to {QUANTIZED_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Serve the quantized model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this in a separate terminal so the notebook can continue:\n",
    "\n",
    "```bash\n",
    "PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \\\n",
    "python scripts/serve_quantized.py \\\n",
    "  --port 8000 --api-key dummy --quantization none \\\n",
    "  --gpu-memory-utilization 0.7 --max-model-len 2048 --max-num-seqs 32\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Smoke test (streaming hello world)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "QUANT_BASE_URL = \"http://localhost:8000/v1\"\n",
    "QUANT_SERVED_MODEL = \"llama-awq-w4a16\"  # update if you used --served-model-name\n",
    "\n",
    "client = OpenAI(base_url=QUANT_BASE_URL, api_key=\"dummy\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=QUANT_SERVED_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello world in one short sentence.\"},\n",
    "    ],\n",
    "    max_tokens=32,\n",
    "    stream=True,\n",
    ")\n",
    "for event in stream:\n",
    "    delta = event.choices[0].delta.content or \"\"\n",
    "    print(delta, end=\"\", flush=True)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run 1000 jobs on the quantized model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} scripts/run_batch.py \\\n",
    "  --base-url http://localhost:8000/v1 \\\n",
    "  --model llama-awq-w4a16 \\\n",
    "  --task xsum --tokenizer llama-awq-w4a16 --max-context-tokens 2048 --context-buffer 128 \\\n",
    "  --output results/awq.jsonl --max-samples 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Serve the non-quantized (base) model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cmd = (\n",
    "    f\"PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \\\n",
    "\"\n",
    "    f\"python scripts/serve_unquantized.py \\\n",
    "\"\n",
    "    f\"  --model {BASE_MODEL_PATH} \\\n",
    "\"\n",
    "    \"  --port 8001 --api-key dummy \\\n",
    "\"\n",
    "    \"  --gpu-memory-utilization 0.7 --max-model-len 2048 --max-num-seqs 16 \\\n",
    "\"\n",
    "    \"  --served-model-name base-llama\"\n",
    ")\n",
    "print(base_cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the quantized server first, then run one of the commands below:\n",
    "\n",
    "```bash\n",
    "# Use the local snapshot to avoid extra HF downloads\n",
    "PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \\\n",
    "python scripts/serve_unquantized.py \\\n",
    "  --model /path/to/local/snapshot \\\n",
    "  --port 8001 --api-key dummy \\\n",
    "  --gpu-memory-utilization 0.7 --max-model-len 2048 --max-num-seqs 16 \\\n",
    "  --served-model-name base-llama\n",
    "\n",
    "# Or pull from HF (requires access + token)\n",
    "PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \\\n",
    "python scripts/serve_unquantized.py \\\n",
    "  --port 8001 --api-key dummy \\\n",
    "  --gpu-memory-utilization 0.7 --max-model-len 2048 --max-num-seqs 16 \\\n",
    "  --served-model-name base-llama\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Smoke test (streaming hello world)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "BASE_BASE_URL = \"http://localhost:8001/v1\"\n",
    "BASE_SERVED_MODEL = \"base-llama\"\n",
    "\n",
    "client = OpenAI(base_url=BASE_BASE_URL, api_key=\"dummy\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=BASE_SERVED_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello world in one short sentence.\"},\n",
    "    ],\n",
    "    max_tokens=32,\n",
    "    stream=True,\n",
    ")\n",
    "for event in stream:\n",
    "    delta = event.choices[0].delta.content or \"\"\n",
    "    print(delta, end=\"\", flush=True)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run 1000 jobs on the base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} scripts/run_batch.py \\\n",
    "  --base-url http://localhost:8001/v1 \\\n",
    "  --model base-llama \\\n",
    "  --task xsum --tokenizer meta-llama/Llama-3.2-1B-Instruct --max-context-tokens 2048 --context-buffer 128 \\\n",
    "  --output results/base.jsonl --max-samples 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Measure accuracy (Rouge-L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa814e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWQ Rouge-L:  0.1552 (1000 samples)\n",
      "Base Rouge-L: 0.1513 (1000 samples)\n",
      "Delta (AWQ - Base): +0.0039\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from statistics import mean\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def load_rows(path: Path):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing results file: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "def rouge_l(rows):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    scores = [\n",
    "        scorer.score(r[\"reference\"], r[\"prediction\"])[\"rougeL\"].fmeasure\n",
    "        for r in rows\n",
    "    ]\n",
    "    return mean(scores)\n",
    "\n",
    "awq_rows = load_rows(Path(\"results/awq.jsonl\"))\n",
    "base_rows = load_rows(Path(\"results/base.jsonl\"))\n",
    "\n",
    "awq_score = rouge_l(awq_rows)\n",
    "base_score = rouge_l(base_rows)\n",
    "\n",
    "print(f\"AWQ Rouge-L:  {awq_score:.4f} ({len(awq_rows)} samples)\")\n",
    "print(f\"Base Rouge-L: {base_score:.4f} ({len(base_rows)} samples)\")\n",
    "print(f\"Delta (AWQ - Base): {awq_score - base_score:+.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
