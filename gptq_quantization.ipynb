{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Instruct 4-bit GPTQ with vLLM Compressor\n\nThis notebook applies 4-bit GPTQ using the vLLM compressor (llm-compressor), serves the model with vLLM, makes a streaming smoke test, runs 1000 examples, and compares accuracy with the base model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n- GPU with compute capability >= 8.0 for fast W4A16 inference\n- Set `HUGGINGFACE_HUB_TOKEN` if the model is gated\n- Restart the kernel after installation if CUDA libraries change\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt (run once per environment)\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q --upgrade -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up Hugging Face access\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Optional: set HUGGINGFACE_HUB_TOKEN in your environment or paste it here.\n",
    "HUGGINGFACE_HUB_TOKEN = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "# HUGGINGFACE_HUB_TOKEN = \"hf_...\"  # Uncomment to hardcode for this notebook session\n",
    "\n",
    "if not HUGGINGFACE_HUB_TOKEN:\n",
    "    print(\"HUGGINGFACE_HUB_TOKEN not set; gated models may fail to load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure model + paths\nGPTQ needs small calibration data to compute quantization parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "CALIBRATION_DATASET = \"HuggingFaceH4/ultrachat_200k\"\n",
    "CALIBRATION_SPLIT = \"train_sft\"\n",
    "NUM_CALIBRATION_SAMPLES = 128  # GPTQ works well with smaller calibration\n",
    "MAX_SEQUENCE_LENGTH = 2048\n",
    "QUANTIZED_DIR = Path(\"llama-gptq-w4a16\")\n",
    "BASE_MODEL_PATH = None  # populated by the download step below\n",
    "\n",
    "QUANTIZED_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Saving quantized model to: {QUANTIZED_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Download the base model (cache snapshot)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_files_only = bool(os.getenv(\"HF_HUB_OFFLINE\")) or not HUGGINGFACE_HUB_TOKEN\n",
    "BASE_MODEL_PATH = snapshot_download(\n",
    "    MODEL_ID,\n",
    "    token=HUGGINGFACE_HUB_TOKEN,\n",
    "    local_files_only=local_files_only,\n",
    ")\n",
    "print(f\"Base model snapshot: {BASE_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load model + tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL_SOURCE = BASE_MODEL_PATH or MODEL_ID\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_SOURCE,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    token=HUGGINGFACE_HUB_TOKEN,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_SOURCE,\n",
    "    trust_remote_code=True,\n",
    "    token=HUGGINGFACE_HUB_TOKEN,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Build calibration set\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_ds = load_dataset(\n",
    "    CALIBRATION_DATASET,\n",
    "    split=f\"{CALIBRATION_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\",\n",
    "    token=HUGGINGFACE_HUB_TOKEN,\n",
    ").shuffle(seed=42)\n",
    "\n",
    "def format_example(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"messages\"], tokenize=False\n",
    "        )\n",
    "    }\n",
    "\n",
    "formatted = raw_ds.map(format_example)\n",
    "\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "calibration_ds = formatted.map(tokenize, remove_columns=formatted.column_names)\n",
    "print(calibration_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Quantize to 4-bit (W4A16) with GPTQ\nGPTQ performs weight-only quantization with calibration data.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization.gptq import GPTQModifier\n",
    "\n",
    "recipe = [\n",
    "    GPTQModifier(\n",
    "        ignore=[\"lm_head\"],\n",
    "        scheme=\"W4A16\",\n",
    "        targets=[\"Linear\"],\n",
    "        block_size=128,\n",
    "        dampening_frac=0.01,\n",
    "    )\n",
    "]\n",
    "\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=calibration_ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "model.save_pretrained(QUANTIZED_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(QUANTIZED_DIR)\n",
    "print(f\"Quantized model saved to {QUANTIZED_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Serve the quantized model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this in a separate terminal so the notebook can continue:\n\n```bash\nPYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \\\npython scripts/serve_vllm.py \\\n  --model llama-gptq-w4a16 --quantization none \\\n  --port 8000 --api-key dummy \\\n  --gpu-memory-utilization 0.7 --max-model-len 2048 --max-num-seqs 32 \\\n  --served-model-name gptq-llama\n```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Smoke test (streaming hello world)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "QUANT_BASE_URL = \"http://localhost:8000/v1\"\n",
    "QUANT_SERVED_MODEL = \"gptq-llama\"  # update if you used --served-model-name\n",
    "\n",
    "client = OpenAI(base_url=QUANT_BASE_URL, api_key=\"dummy\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=QUANT_SERVED_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello world in one short sentence.\"},\n",
    "    ],\n",
    "    max_tokens=32,\n",
    "    stream=True,\n",
    ")\n",
    "for event in stream:\n",
    "    delta = event.choices[0].delta.content or \"\"\n",
    "    print(delta, end=\"\", flush=True)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run 1000 jobs on the quantized model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} scripts/run_batch.py \\\n",
    "  --base-url http://localhost:8000/v1 \\\n",
    "  --model gptq-llama \\\n",
    "  --task xsum --tokenizer llama-gptq-w4a16 --max-context-tokens 2048 --context-buffer 128 \\\n",
    "  --output results/gptq.jsonl --max-samples 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Serve the non-quantized (base) model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_cmd = (\n",
    "    f\"PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \\\\\n\"\n",
    "    f\"python scripts/serve_unquantized.py \\\\\n\"\n",
    "    f\"  --model {BASE_MODEL_PATH} \\\\\n\"\n",
    "    \"  --port 8001 --api-key dummy \\\\\n\"\n",
    "    \"  --gpu-memory-utilization 0.7 --max-model-len 2048 --max-num-seqs 16 \\\\\n\"\n",
    "    \"  --served-model-name base-llama\"\n",
    ")\n",
    "print(base_cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the quantized server first, then run one of the commands below:\n\n```bash\n# Use the local snapshot to avoid extra HF downloads\nPYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \\\npython scripts/serve_unquantized.py \\\n  --model /path/to/local/snapshot \\\n  --port 8001 --api-key dummy \\\n  --gpu-memory-utilization 0.7 --max-model-len 2048 --max-num-seqs 16 \\\n  --served-model-name base-llama\n\n# Or pull from HF (requires access + token)\nPYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \\\npython scripts/serve_unquantized.py \\\n  --port 8001 --api-key dummy \\\n  --gpu-memory-utilization 0.7 --max-model-len 2048 --max-num-seqs 16 \\\n  --served-model-name base-llama\n```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Smoke test (streaming hello world)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "BASE_BASE_URL = \"http://localhost:8001/v1\"\n",
    "BASE_SERVED_MODEL = \"base-llama\"\n",
    "\n",
    "client = OpenAI(base_url=BASE_BASE_URL, api_key=\"dummy\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=BASE_SERVED_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello world in one short sentence.\"},\n",
    "    ],\n",
    "    max_tokens=32,\n",
    "    stream=True,\n",
    ")\n",
    "for event in stream:\n",
    "    delta = event.choices[0].delta.content or \"\"\n",
    "    print(delta, end=\"\", flush=True)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run 1000 jobs on the base model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} scripts/run_batch.py \\\n",
    "  --base-url http://localhost:8001/v1 \\\n",
    "  --model base-llama \\\n",
    "  --task xsum --tokenizer meta-llama/Llama-3.2-1B-Instruct --max-context-tokens 2048 --context-buffer 128 \\\n",
    "  --output results/base.jsonl --max-samples 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Measure accuracy (Rouge-L)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from statistics import mean\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def load_rows(path: Path):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing results file: {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "def rouge_l(rows):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    scores = [\n",
    "        scorer.score(r[\"reference\"], r[\"prediction\"])[\"rougeL\"].fmeasure\n",
    "        for r in rows\n",
    "    ]\n",
    "    return mean(scores)\n",
    "\n",
    "gptq_rows = load_rows(Path(\"results/gptq.jsonl\"))\n",
    "base_rows = load_rows(Path(\"results/base.jsonl\"))\n",
    "\n",
    "gptq_score = rouge_l(gptq_rows)\n",
    "base_score = rouge_l(base_rows)\n",
    "\n",
    "print(f\"GPTQ Rouge-L: {gptq_score:.4f} ({len(gptq_rows)} samples)\")\n",
    "print(f\"Base Rouge-L: {base_score:.4f} ({len(base_rows)} samples)\")\n",
    "print(f\"Delta (GPTQ - Base): {gptq_score - base_score:+.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
