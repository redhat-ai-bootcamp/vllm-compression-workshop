{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0e77ab",
   "metadata": {},
   "source": [
    "# Llama Instruct 4-bit GPTQ with vLLM Compressor\n",
    "\n",
    "This notebook walks through 4-bit weight-only GPTQ quantization with the vLLM compressor (llm-compressor), serves the quantized model with vLLM, makes a sample call, and benchmarks accuracy on 100 samples. Run the cells in order; any cell with `!` executes a shell command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcedb7c",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- GPU with compute capability >= 8.0 (Ampere or newer) for fast W4A16 inference\n",
    "- Enough VRAM for the chosen model (quantized model still needs ~40-50% of BF16 size)\n",
    "- Set `HUGGINGFACE_HUB_TOKEN` in the environment if the model is gated\n",
    "\n",
    "If you are rerunning, restart the kernel after installation to ensure the right CUDA/toolkit versions are picked up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f3d8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vLLM + llm-compressor and friends (pin llmcompressor 0.6.0.1: py3.12-friendly and numpy<2/vLLM compatible)\n",
    "!pip install -q --upgrade \\\n",
    "  'vllm>=0.5.5' \\\n",
    "  'llmcompressor==0.6.0.1' \\\n",
    "  'transformers>=4.43' \\\n",
    "  datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd88fe1",
   "metadata": {},
   "source": [
    "## Configure model + paths\n",
    "Pick any Llama Instruct variant that you have access to. Calibration uses a small slice of UltraChat by default to keep the run quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc29519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/vllm/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving quantized model to: /home/ubuntu/vllm/llama-gptq-w4a16\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\"  # change if you prefer another model\n",
    "CALIBRATION_DATASET = \"HuggingFaceH4/ultrachat_200k\"\n",
    "CALIBRATION_SPLIT = \"train_sft\"\n",
    "NUM_CALIBRATION_SAMPLES = 128  # increase to 512+ for better quality\n",
    "MAX_SEQUENCE_LENGTH = 2048\n",
    "QUANTIZED_DIR = Path(\"llama-gptq-w4a16\")\n",
    "\n",
    "QUANTIZED_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Saving quantized model to: {QUANTIZED_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d0cd96",
   "metadata": {},
   "source": [
    "## Load model + tokenizer\n",
    "Using `torch_dtype='auto'` keeps memory reasonable during quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44190392",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c7a5d",
   "metadata": {},
   "source": [
    "## Build calibration set\n",
    "We keep preprocessing simple: apply the chat template, shuffle, then tokenize without adding extra special tokens (the template already includes BOS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and shuffle a small calibration slice\n",
    "raw_ds = load_dataset(\n",
    "    CALIBRATION_DATASET,\n",
    "    split=f\"{CALIBRATION_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]\",\n",
    ").shuffle(seed=42)\n",
    "\n",
    "def format_example(example):\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"messages\"], tokenize=False\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Apply chat template then tokenize\n",
    "formatted = raw_ds.map(format_example)\n",
    "\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "calibration_ds = formatted.map(tokenize, remove_columns=formatted.column_names)\n",
    "print(calibration_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771b76d",
   "metadata": {},
   "source": [
    "## Quantize to 4-bit (W4A16) with GPTQ\n",
    "This uses the vLLM compressor one-shot pipeline with the built-in GPTQ recipe for 4-bit weights (group size 128). Increase `num_calibration_samples` for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60098f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "\n",
    "recipe = GPTQModifier(targets=\"Linear\", scheme=\"W4A16\", ignore=[\"lm_head\"])\n",
    "\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=calibration_ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")\n",
    "\n",
    "model.save_pretrained(QUANTIZED_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(QUANTIZED_DIR)\n",
    "print(f\"Quantized model saved to {QUANTIZED_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02f34d5",
   "metadata": {},
   "source": [
    "## Serve with vLLM\n",
    "Start the OpenAI-compatible server in a terminal (or a background cell). Update `--tensor-parallel-size` if using multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8270d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: launch the server\n",
    "# !CUDA_VISIBLE_DEVICES=0 vllm serve \"$QUANTIZED_DIR\" \\\n",
    "#   --max-model-len 4096 \\\n",
    "#   --tensor-parallel-size 1 \\\n",
    "#   --port 8000 \\\n",
    "#   --api-key dummy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bdb702",
   "metadata": {},
   "source": [
    "## Call the served model\n",
    "Uses the OpenAI client pointed at the local vLLM server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13752102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"dummy\")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gptq-w4a16-demo\",  # the model id exposed by vLLM (use GET /v1/models to inspect)\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me two bullet points on why quantization helps inference.\"},\n",
    "    ],\n",
    "    max_tokens=64,\n",
    ")\n",
    "print(resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b40ae",
   "metadata": {},
   "source": [
    "## Local inference without the server (optional)\n",
    "You can also load the quantized checkpoint directly with the Python API for quick experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061dcab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "sampling = SamplingParams(temperature=0.0, max_tokens=64)\n",
    "local_llm = LLM(model=str(QUANTIZED_DIR), tensor_parallel_size=1)\n",
    "outputs = local_llm.generate([\n",
    "    \"Summarize quantization in one sentence.\",\n",
    "], sampling)\n",
    "print(outputs[0].outputs[0].text.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242c585",
   "metadata": {},
   "source": [
    "## Benchmark on a small dataset (100 samples)\n",
    "We use `tweet_eval/sentiment` (3-way classification). The prompt asks for a one-word label, and we compute exact-match accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01939f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "LABELS = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "EVAL_SPLIT = \"train[:100]\"\n",
    "\n",
    "eval_ds = load_dataset(\"tweet_eval\", \"sentiment\", split=EVAL_SPLIT)\n",
    "\n",
    "prompts = [\n",
    "    f\"\"\"Classify the sentiment of the tweet as negative, neutral, or positive. Respond with a single word.\n",
    "\n",
    "Tweet: {row['text']}\n",
    "\n",
    "Label:\"\"\"\n",
    "    for row in eval_ds\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=3, stop=[\"\n",
    "\"])\n",
    "llm_for_eval = local_llm  # reuse the in-memory quantized model\n",
    "\n",
    "generations = llm_for_eval.generate(prompts, sampling_params)\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    t = text.strip().lower()\n",
    "    for lbl in LABELS.values():\n",
    "        if lbl in t:\n",
    "            return lbl\n",
    "    if t.startswith(\"pos\"):\n",
    "        return \"positive\"\n",
    "    if t.startswith(\"neg\"):\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "preds = [normalize(out.outputs[0].text) for out in generations]\n",
    "gold = [LABELS[int(row[\"label\"])] for row in eval_ds]\n",
    "\n",
    "acc = np.mean([p == g for p, g in zip(preds, gold)])\n",
    "counts = Counter(preds)\n",
    "print(f\"Accuracy on {len(gold)} samples: {acc:.3f}\")\n",
    "print(\"Prediction distribution:\", counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
